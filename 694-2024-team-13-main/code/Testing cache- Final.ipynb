{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c0ae917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\lpnhu\\anaconda3\\lib\\site-packages (4.6.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\lpnhu\\anaconda3\\lib\\site-packages (from pymongo) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b5e2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from lrucache import LRUCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb8313ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URL=\"mongodb://localhost:27017\"\n",
    "client=MongoClient(MONGODB_URL)\n",
    "tweets_collection= client.dbms_project.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15211ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the entire collection\n",
    "# tweets_collection.delete_many({})\n",
    "# print(\"Collection cleared. Ready to start anew.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "512f4ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tweet_id_1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so that duplicate documents dont get inserted(raises an error)\n",
    "tweets_collection.create_index(\"tweet_id\", unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a80ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are reading the contents of the original tweet if the tweet is retweeted \n",
    "#for access to extended tweeet if available\n",
    "def get_tweet_text(data):\n",
    "    # Check if the tweet is a retweet based on the text content\n",
    "    if data['text'].startswith('RT'):\n",
    "        # Retrieve the original tweet's data from the retweeted_status, if available\n",
    "        retweet = data.get('retweeted_status', {})\n",
    "        if 'extended_tweet' in retweet:\n",
    "            # Use full_text from extended_tweet if available\n",
    "            return retweet['extended_tweet']['full_text']\n",
    "        else:\n",
    "            # Use text from retweeted_status if extended_tweet is not available\n",
    "            return retweet.get('text', data['text'])\n",
    "    else:\n",
    "        # For a non-retweet, check if it's an extended tweet\n",
    "        if 'extended_tweet' in data:\n",
    "            # Use full_text from extended_tweet if available\n",
    "            return data['extended_tweet']['full_text']\n",
    "        else:\n",
    "            # Use standard text field if it's not an extended tweet\n",
    "            return data['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46f2b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(data):\n",
    "    # Check if the tweet is a retweet and extract hashtags accordingly\n",
    "    if data['text'].startswith('RT') and 'retweeted_status' in data:\n",
    "        retweet = data['retweeted_status']\n",
    "        hashtags_list = retweet.get('extended_tweet', {}).get('entities', {}).get('hashtags', retweet.get('entities', {}).get('hashtags', []))\n",
    "    else:\n",
    "        hashtags_list = data.get('extended_tweet', {}).get('entities', {}).get('hashtags', data.get('entities', {}).get('hashtags', []))\n",
    "    return [hashtag['text'] for hashtag in hashtags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ff78c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_insert(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if tweets_collection.count_documents({\"tweet_id\": data[\"id\"]}) == 0:\n",
    "                    tweet_text = get_tweet_text(data)\n",
    "                    hashtags = get_hashtags(data)\n",
    "\n",
    "                    tweet_document = {\n",
    "                        \"tweet_id\": data[\"id\"],\n",
    "                        \"text\": tweet_text,\n",
    "                        \"hashtags\": hashtags,\n",
    "                        \"user\": {\n",
    "                            \"user_id\": data['user']['id'],\n",
    "                            \"name\": data['user']['name'],\n",
    "                            \"screen_name\": data['user']['screen_name']\n",
    "                        },\n",
    "                        \"created_at\": parse_date(data['created_at'])\n",
    "                    }\n",
    "\n",
    "                    tweets_collection.insert_one(tweet_document)\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                continue  # Skip invalid or incomplete lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9637ecd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/lpnhu/Downloads/694-2024-team-13/data/corona-out-2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m file_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/lpnhu/Downloads/694-2024-team-13/data/corona-out-3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Process each file\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m read_and_insert(file_1)\n\u001b[0;32m     10\u001b[0m read_and_insert(file_2)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments inserted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m, in \u001b[0;36mread_and_insert\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_and_insert\u001b[39m(file_name):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/lpnhu/Downloads/694-2024-team-13/data/corona-out-2'"
     ]
    }
   ],
   "source": [
    "def parse_date(date_str):\n",
    "    return datetime.strptime(date_str, '%a %b %d %H:%M:%S %z %Y')\n",
    "\n",
    "# File paths\n",
    "file_1 = \"C:/Users/lpnhu/Downloads/694-2024-team-13/data/corona-out-2\"\n",
    "file_2 = \"C:/Users/lpnhu/Downloads/694-2024-team-13/data/corona-out-3\"\n",
    "\n",
    "# Process each file\n",
    "read_and_insert(file_1)\n",
    "read_and_insert(file_2)\n",
    "\n",
    "print(\"Documents inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c4da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54d515bb",
   "metadata": {},
   "source": [
    "# Integrate cache with MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "446f2893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file not found, starting with an empty cache.\n"
     ]
    }
   ],
   "source": [
    "tweets_collection = client.dbms_project.tweets\n",
    "\n",
    "def fetch_tweet_from_mongodb(tweet_id):\n",
    "    tweet = tweets_collection.find_one({\"tweet_id\": tweet_id})\n",
    "    return tweet\n",
    "\n",
    "cache = LRUCache(capacity=100, ttl=3600, persistence_path='cache.json')\n",
    "cache.restore()\n",
    "\n",
    "# Modify the get method in LRUCache to fetch data from MongoDB if not in cache\n",
    "# Define the fallback function outside the LRUCache class\n",
    "def get_with_mongo_fallback(self, key):\n",
    "    data = LRUCache.get(self, key)  # Call the original get method\n",
    "    if data is None:\n",
    "        # If not in cache, fetch from MongoDB\n",
    "        data = fetch_tweet_from_mongodb(key)\n",
    "        if data is not None:\n",
    "            # Update the cache with the fetched data\n",
    "            self.put(key, data)\n",
    "    return data\n",
    "\n",
    "# Bind the new method to the cache instance, bypassing the overridden get method\n",
    "import types\n",
    "cache.get = types.MethodType(get_with_mongo_fallback, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b86e6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "tweet_id = '1254022772558368768'\n",
    "tweet_data = cache.get(tweet_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43760fae",
   "metadata": {},
   "source": [
    "# Set up timing and logging mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfdb2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up basic logging to a file\n",
    "logging.basicConfig(filename='cache_performance.log', level=logging.INFO)\n",
    "\n",
    "def log_performance(start_time, end_time, operation, key, hit_or_miss):\n",
    "    duration = (end_time - start_time) * 1000 #measure in milliseconds\n",
    "    logging.info(f\"{operation} took {duration:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c64cab",
   "metadata": {},
   "source": [
    "# Modify cache methods to include timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c37e189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_with_mongo_fallback(self, key):\n",
    "        start_time = time.perf_counter()  \n",
    "        data = super().get(key)  \n",
    "        if data is not None:\n",
    "            # Cache hit, Log the performance\n",
    "            end_time = time.perf_counter()  \n",
    "            log_performance(start_time, end_time, \"Cache hit\", key, \"hit\")\n",
    "        else:\n",
    "            # Cache miss, fetch from MongoDB and then put it in the cache\n",
    "            data = fetch_tweet_from_mongodb(key) \n",
    "            if data is not None:\n",
    "                self.put(key, data)\n",
    "            end_time = time.perf_counter()  # Change to use perf_counter\n",
    "            log_performance(start_time, end_time, \"MongoDB fetch\", key, \"miss\")\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39ce5284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file not found, starting with an empty cache.\n"
     ]
    }
   ],
   "source": [
    "# Replace the get method in the LRUCache instance\n",
    "import types\n",
    "cache = LRUCache(capacity=100, ttl=3600, persistence_path='cache.json')\n",
    "cache.restore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4991851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the cache\n",
    "def test_cache_performance(cache, test_keys):\n",
    "    for key in test_keys:\n",
    "        # First access will always be a miss since we're not assuming pre-loading\n",
    "        start_time = time.perf_counter()\n",
    "        data = cache.get(key)\n",
    "        end_time = time.perf_counter()\n",
    "        log_performance(start_time, end_time, \"Access\", key, \"miss\")\n",
    "\n",
    "        # Subsequent accesses should be hits if the key is still in the cache\n",
    "        for _ in range(3):  # Access the same key three times to test cache hits\n",
    "            start_time = time.perf_counter()\n",
    "            data = cache.get(key)\n",
    "            end_time = time.perf_counter()\n",
    "            log_performance(start_time, end_time, \"Access\", key, \"hit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85ecae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test_keys taken from the MongoDB \n",
    "test_keys = [\n",
    "    '1249403767108668930', \n",
    "    '1249403768023678982', \n",
    "    '1249403769193779202'\n",
    "]\n",
    "\n",
    "test_cache_performance(cache, test_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138af19",
   "metadata": {},
   "source": [
    "# Testing for cache miss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='cache_performance.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68abc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_performance(start_time, end_time, operation, key, hit_or_miss):\n",
    "    duration = end_time - start_time\n",
    "    logging.info(f\"{operation} for key {key} ({hit_or_miss}) took {duration:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28e9e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cache_miss_timing(cache, test_keys, fetch_from_db):\n",
    "    for key in test_keys:\n",
    "        # Clear the key from the cache to ensure a cache miss\n",
    "        cache.cache.pop(key, None)\n",
    "        \n",
    "        # Now access the key, which should trigger a cache miss and a database fetch\n",
    "        start_time = time.perf_counter()\n",
    "        data = fetch_from_db(key)\n",
    "        \n",
    "        # Assume the fetch_from_db function updates the cache after a miss\n",
    "        cache.put(key, data)\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        log_performance(start_time, end_time, \"Database fetch\", key, \"miss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62b5f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_from_db(key):\n",
    "    # Simulate a database fetch with a sleep\n",
    "    time.sleep(0.01)  # Simulate database latency\n",
    "    # Fetch the data from the database here (this is just a placeholder)\n",
    "    return \"data_from_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40e83680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cache\n",
    "cache = LRUCache(capacity=100, ttl=3600, persistence_path='cache.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c85ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test for cache misses\n",
    "test_keys = [\n",
    "    '1249403767108668930', \n",
    "    '1249403768023678982', \n",
    "    '1249403769193779202'\n",
    "]\n",
    "\n",
    "test_cache_miss_timing(cache, test_keys, fetch_from_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a085cd08",
   "metadata": {},
   "source": [
    "# Testing for cache hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d83a9009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file not found, starting with an empty cache.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='cache_performance.log', level=logging.INFO)\n",
    "\n",
    "def log_performance(start_time, end_time, operation, key, hit_or_miss):\n",
    "    # Calculate duration in milliseconds\n",
    "    duration = (end_time - start_time) * 1000  # Convert from seconds to milliseconds\n",
    "    logging.info(f\"{operation} for key {key} ({hit_or_miss}) took {duration:.3f} ms\")\n",
    "\n",
    "def test_cache_hit_timing(cache, test_keys, fetch_from_db):\n",
    "    for key in test_keys:\n",
    "        # Ensure the key is in the cache by fetching it from the db if it's not already there\n",
    "        if key not in cache.cache:\n",
    "            data = fetch_from_db(key)\n",
    "            cache.put(key, data)\n",
    "\n",
    "        # Now access the key, which should be a cache hit\n",
    "        start_time = time.perf_counter()\n",
    "        data = cache.get(key)  # Should be a cache hit as the data is already in the cache\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        log_performance(start_time, end_time, \"Cache access\", key, \"hit\")\n",
    "\n",
    "def fetch_from_db(key):\n",
    "    # Simulate a database fetch with a sleep\n",
    "    time.sleep(0.01)  # Simulate database latency for the initial fetch\n",
    "    return \"data_from_db\"\n",
    "\n",
    "# Initialize the cache\n",
    "cache = LRUCache(capacity=100, ttl=3600, persistence_path='cache.json')\n",
    "cache.restore()\n",
    "\n",
    "# Example test keys for the test\n",
    "test_keys = [\n",
    "    '1249403767108668930', \n",
    "    '1249403768023678982', \n",
    "    '1249403769193779202'\n",
    "]\n",
    "\n",
    "# Run the test for cache hits\n",
    "test_cache_hit_timing(cache, test_keys, fetch_from_db)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
